#!/usr/bin/env python3
"""
Process the full 15-year ES dataset with weighted labeling and feature engineering
This is the production version designed for large-scale processing
"""
import sys
import os
import time
import boto3
import pandas as pd
from pathlib import Path
from datetime import datetime
import psutil

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def check_system_requirements():
    """Check if system has enough resources for full dataset processing"""
    print("üîç CHECKING SYSTEM REQUIREMENTS")
    print("=" * 50)
    
    # Check memory
    memory = psutil.virtual_memory()
    memory_gb = memory.total / (1024**3)
    available_gb = memory.available / (1024**3)
    
    print(f"üíæ Memory: {memory_gb:.1f} GB total, {available_gb:.1f} GB available")
    
    if memory_gb < 32:
        print("‚ö†Ô∏è  Warning: Less than 32 GB RAM. Consider upgrading instance.")
    if available_gb < 16:
        print("‚ùå Error: Less than 16 GB available. Close other processes.")
        return False
    
    # Check disk space
    disk = psutil.disk_usage('/')
    disk_gb = disk.total / (1024**3)
    free_gb = disk.free / (1024**3)
    
    print(f"üíΩ Disk: {disk_gb:.1f} GB total, {free_gb:.1f} GB free")
    
    if free_gb < 100:
        print("‚ùå Error: Less than 100 GB free space needed.")
        return False
    
    # Check CPU
    cpu_count = psutil.cpu_count()
    print(f"üñ•Ô∏è  CPU: {cpu_count} cores")
    
    print("‚úÖ System requirements check passed")
    return True

def download_full_dataset():
    """Download the full 15-year dataset from S3"""
    print("\nüì• DOWNLOADING FULL DATASET")
    print("=" * 50)
    
    # S3 configuration for full dataset
    bucket_name = "your-full-dataset-bucket"  # Update with actual bucket
    s3_key = "raw-data/es_15year_data.dbn.zst"  # Update with actual key
    local_file = Path("/tmp/es_full_processing/es_15year_data.dbn.zst")
    
    # Create work directory
    work_dir = local_file.parent
    work_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"üì§ S3: s3://{bucket_name}/{s3_key}")
    print(f"üì• Local: {local_file}")
    
    if local_file.exists():
        size_gb = local_file.stat().st_size / (1024**3)
        print(f"‚úÖ File already exists ({size_gb:.1f} GB)")
        return str(local_file)
    
    try:
        s3_client = boto3.client('s3')
        
        # Get file size first
        response = s3_client.head_object(Bucket=bucket_name, Key=s3_key)
        file_size = response['ContentLength']
        file_size_gb = file_size / (1024**3)
        
        print(f"üìä File size: {file_size_gb:.1f} GB")
        print("üì• Starting download (this will take 30-60 minutes)...")
        
        start_time = time.time()
        
        # Download with progress callback
        def progress_callback(bytes_transferred):
            percent = (bytes_transferred / file_size) * 100
            elapsed = time.time() - start_time
            if elapsed > 0:
                speed_mbps = (bytes_transferred / (1024**2)) / elapsed
                print(f"   Progress: {percent:.1f}% ({speed_mbps:.1f} MB/s)", end='\r')
        
        s3_client.download_file(
            bucket_name, 
            s3_key, 
            str(local_file),
            Callback=progress_callback
        )
        
        elapsed = time.time() - start_time
        print(f"\n‚úÖ Download complete in {elapsed/60:.1f} minutes")
        
        return str(local_file)
        
    except Exception as e:
        print(f"‚ùå Download failed: {e}")
        return None

def process_full_dataset_chunked():
    """Process the full dataset in chunks to manage memory"""
    print("\nüîÑ PROCESSING FULL DATASET")
    print("=" * 50)
    
    # File paths
    input_file = Path("/tmp/es_full_processing/es_15year_data.dbn.zst")
    output_dir = Path("/tmp/es_full_processing/processed_chunks")
    final_output = Path("/tmp/es_full_processing/es_15year_labeled_features.parquet")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if not input_file.exists():
        print(f"‚ùå Input file not found: {input_file}")
        return None
    
    try:
        # Import processing modules
        from src.data_pipeline.weighted_labeling import WeightedLabelingEngine
        from src.data_pipeline.features import create_all_features
        
        print("‚úÖ Processing modules imported")
        
        # Initialize processing engine
        engine = WeightedLabelingEngine()
        
        # Process in chunks (adjust chunk size based on available memory)
        chunk_size = 1_000_000  # 1M rows per chunk
        chunk_files = []
        
        print(f"üìä Processing in chunks of {chunk_size:,} rows")
        print("‚è≥ This will take several hours for the full dataset...")
        
        # Open DBN store
        import databento as db
        store = db.DBNStore.from_file(str(input_file))
        
        # Get total rows estimate
        metadata = store.metadata
        print(f"üìÖ Date range: {metadata.start} to {metadata.end}")
        
        # Process chunks
        chunk_num = 0
        total_processed = 0
        start_time = time.time()
        
        # This is a simplified version - you'd need to implement proper chunked reading
        # For now, let's process the full dataset if memory allows
        print("üîÑ Converting DBN to DataFrame...")
        df = store.to_df()
        
        # Add timestamps
        if hasattr(df.index, 'astype'):
            df.index = pd.to_datetime(df.index, unit='ns', utc=True)
            df.index.name = 'timestamp'
        
        # Convert to column format
        df = df.reset_index()
        
        total_rows = len(df)
        print(f"üìä Total rows: {total_rows:,}")
        
        # Process in chunks
        for start_idx in range(0, total_rows, chunk_size):
            end_idx = min(start_idx + chunk_size, total_rows)
            chunk_df = df.iloc[start_idx:end_idx].copy()
            
            chunk_num += 1
            print(f"\nüîÑ Processing chunk {chunk_num} (rows {start_idx:,}-{end_idx:,})")
            
            # Apply weighted labeling
            print("   üè∑Ô∏è  Weighted labeling...")
            labeled_df = engine.process_dataframe(chunk_df, validate_performance=False)
            
            # Apply feature engineering
            print("   üîß Feature engineering...")
            final_df = create_all_features(labeled_df)
            
            # Save chunk
            chunk_file = output_dir / f"chunk_{chunk_num:04d}.parquet"
            final_df.to_parquet(chunk_file, index=False)
            chunk_files.append(chunk_file)
            
            total_processed += len(final_df)
            elapsed = time.time() - start_time
            rate = total_processed / elapsed if elapsed > 0 else 0
            
            print(f"   ‚úÖ Chunk {chunk_num} complete ({len(final_df):,} rows)")
            print(f"   üìä Progress: {total_processed:,}/{total_rows:,} ({total_processed/total_rows*100:.1f}%)")
            print(f"   ‚è±Ô∏è  Rate: {rate:.0f} rows/second")
            
            # Memory cleanup
            del chunk_df, labeled_df, final_df
            
        # Combine all chunks
        print(f"\nüîó COMBINING {len(chunk_files)} CHUNKS")
        print("-" * 40)
        
        combined_dfs = []
        for chunk_file in chunk_files:
            chunk_df = pd.read_parquet(chunk_file)
            combined_dfs.append(chunk_df)
            print(f"   Loaded {chunk_file.name}: {len(chunk_df):,} rows")
        
        # Concatenate all chunks
        print("üîó Concatenating chunks...")
        final_df = pd.concat(combined_dfs, ignore_index=True)
        
        # Save final result
        print("üíæ Saving final dataset...")
        final_df.to_parquet(final_output, index=False)
        
        # Cleanup chunk files
        print("üßπ Cleaning up chunk files...")
        for chunk_file in chunk_files:
            chunk_file.unlink()
        output_dir.rmdir()
        
        total_time = time.time() - start_time
        file_size_gb = final_output.stat().st_size / (1024**3)
        
        print(f"\nüéâ PROCESSING COMPLETE!")
        print(f"   Output: {final_output}")
        print(f"   Rows: {len(final_df):,}")
        print(f"   Columns: {len(final_df.columns)}")
        print(f"   File size: {file_size_gb:.1f} GB")
        print(f"   Total time: {total_time/3600:.1f} hours")
        
        return str(final_output)
        
    except Exception as e:
        print(f"‚ùå Processing failed: {e}")
        import traceback
        traceback.print_exc()
        return None

def upload_full_dataset_results(processed_file):
    """Upload the processed full dataset to S3"""
    print("\nüì§ UPLOADING RESULTS TO S3")
    print("=" * 50)
    
    if not processed_file or not Path(processed_file).exists():
        print("‚ùå No processed file to upload")
        return False
    
    try:
        s3_client = boto3.client('s3')
        bucket_name = "your-results-bucket"  # Update with actual bucket
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        s3_key = f"processed-data/es_15year_labeled_features_{timestamp}.parquet"
        
        file_size_gb = Path(processed_file).stat().st_size / (1024**3)
        
        print(f"üì• Local: {processed_file}")
        print(f"üì§ S3: s3://{bucket_name}/{s3_key}")
        print(f"üìä Size: {file_size_gb:.1f} GB")
        print("‚è≥ Upload will take 30-60 minutes...")
        
        start_time = time.time()
        
        # Upload with multipart for large files
        s3_client.upload_file(
            processed_file,
            bucket_name,
            s3_key,
            ExtraArgs={
                'Metadata': {
                    'source': 'full_dataset_processing',
                    'processing_date': timestamp,
                    'dataset': '15_year_es_data'
                }
            }
        )
        
        upload_time = time.time() - start_time
        
        print(f"‚úÖ Upload complete in {upload_time/60:.1f} minutes")
        print(f"üì§ S3 URL: s3://{bucket_name}/{s3_key}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Upload failed: {e}")
        return False

def main():
    """Main processing pipeline for full dataset"""
    print("üöÄ FULL DATASET PROCESSING PIPELINE")
    print("=" * 60)
    print("‚ö†Ô∏è  This will process the complete 15-year ES dataset")
    print("‚ö†Ô∏è  Estimated time: 6-12 hours")
    print("‚ö†Ô∏è  Estimated output size: 50-100 GB")
    print()
    
    # Confirm before proceeding
    response = input("Continue with full dataset processing? (yes/no): ")
    if response.lower() != 'yes':
        print("‚ùå Processing cancelled")
        return
    
    # Check system requirements
    if not check_system_requirements():
        print("‚ùå System requirements not met")
        return
    
    # Download full dataset
    input_file = download_full_dataset()
    if not input_file:
        print("‚ùå Failed to download dataset")
        return
    
    # Process full dataset
    processed_file = process_full_dataset_chunked()
    if not processed_file:
        print("‚ùå Failed to process dataset")
        return
    
    # Upload results
    if upload_full_dataset_results(processed_file):
        print("\nüéâ FULL DATASET PROCESSING COMPLETE!")
        print("üöÄ Ready for XGBoost model training on 15 years of data!")
    else:
        print("\n‚ö†Ô∏è  Processing complete but upload failed")
        print(f"Manual upload needed: {processed_file}")

if __name__ == "__main__":
    main()