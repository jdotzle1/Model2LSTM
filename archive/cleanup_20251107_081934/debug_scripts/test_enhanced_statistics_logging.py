#!/usr/bin/env python3
"""
Test Enhanced Statistics Logging and S3 Metadata System

This test validates the comprehensive statistics collection, quality scoring,
and reporting system implemented for task 4.
"""

import sys
import os
import tempfile
import json
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.data_pipeline.monthly_statistics import (
    MonthlyProcessingStatistics,
    MonthlyStatisticsCollector,
    QualityScorer,
    MultiMonthReportGenerator,
    RolloverEvent,
    ModeStatistics,
    FeatureStatistics,
    DataQualityMetrics,
    ProcessingPerformanceMetrics,
    validate_monthly_quality,
    create_monthly_quality_report,
    save_monthly_report,
    save_summary_report
)


def create_sample_monthly_statistics(month_str: str, quality_level: str = "good") -> MonthlyProcessingStatistics:
    """Create sample monthly statistics for testing"""
    
    # Create sample mode statistics
    mode_statistics = {}
    for mode_name in ['low_vol_long', 'normal_vol_long', 'high_vol_long', 
                      'low_vol_short', 'normal_vol_short', 'high_vol_short']:
        
        if quality_level == "good":
            win_rate = 0.25  # Good win rate
            validation_passed = True
            quality_flags = []
        elif quality_level == "poor":
            win_rate = 0.75  # Poor win rate (too high)
            validation_passed = False
            quality_flags = ['unreasonable_win_rate', 'validation_failed']
        else:  # medium
            win_rate = 0.15  # Acceptable but low
            validation_passed = True
            quality_flags = ['low_win_rate']
        
        mode_statistics[mode_name] = ModeStatistics(
            win_rate=win_rate,
            total_winners=int(win_rate * 10000),
            total_samples=10000,
            loss_rate=1.0 - win_rate,
            avg_weight=1.5,
            weight_std=0.8,
            min_weight=0.5,
            max_weight=4.0,
            weight_percentiles={'p25': 1.0, 'p50': 1.5, 'p75': 2.0, 'p90': 2.5, 'p95': 3.0},
            nan_percentage_labels=0.0,
            nan_percentage_weights=0.0,
            has_nan_labels=False,
            has_nan_weights=False,
            has_infinite_labels=False,
            has_infinite_weights=False,
            labels_binary=True,
            weights_positive=True,
            win_rate_reasonable=(0.05 <= win_rate <= 0.50),
            validation_passed=validation_passed,
            quality_flags=quality_flags
        )
    
    # Create sample feature statistics
    if quality_level == "good":
        feature_quality_score = 0.85
        features_generated = 43
        high_nan_features = []
    elif quality_level == "poor":
        feature_quality_score = 0.45
        features_generated = 35
        high_nan_features = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6']
    else:  # medium
        feature_quality_score = 0.65
        features_generated = 41
        high_nan_features = ['feature1', 'feature2']
    
    feature_statistics = FeatureStatistics(
        features_generated=features_generated,
        expected_features=43,
        feature_completeness=(features_generated / 43) * 100,
        nan_percentages={'feature1': 25.0, 'feature2': 30.0} if quality_level != "good" else {},
        high_nan_features=high_nan_features,
        avg_nan_percentage=15.0 if quality_level == "poor" else 5.0,
        max_nan_percentage=30.0 if quality_level == "poor" else 10.0,
        outlier_counts={'feature3': 100} if quality_level == "poor" else {},
        features_with_outliers=['feature3'] if quality_level == "poor" else [],
        value_ranges={'feature1': (0.0, 1.0), 'feature2': (-1.0, 1.0)},
        suspicious_ranges=['feature4'] if quality_level == "poor" else [],
        quality_score=feature_quality_score
    )
    
    # Create sample data quality metrics
    if quality_level == "good":
        retention_rate = 0.75
        excessive_data_loss = False
        warnings = []
    elif quality_level == "poor":
        retention_rate = 0.25
        excessive_data_loss = True
        warnings = ['high_data_loss', 'unusual_rth_percentage', 'price_issues']
    else:  # medium
        retention_rate = 0.55
        excessive_data_loss = False
        warnings = ['minor_data_issues']
    
    data_quality = DataQualityMetrics(
        raw_bars=100000,
        cleaned_bars=95000,
        rth_bars=int(95000 * 0.4),  # ~40% RTH
        final_bars=int(100000 * retention_rate),
        cleaning_retention_rate=95.0,
        rth_retention_rate=40.0,
        final_retention_rate=retention_rate * 100,
        overall_retention_rate=retention_rate * 100,
        price_issues_fixed=50 if quality_level == "poor" else 10,
        negative_volume_fixed=5,
        invalid_ohlc_fixed=3,
        timezone_issues=2,
        excessive_data_loss=excessive_data_loss,
        unusual_rth_percentage=quality_level == "poor",
        data_quality_warnings=warnings
    )
    
    # Create sample performance metrics
    if quality_level == "good":
        processing_time = 25.0
        peak_memory = 5500.0
    elif quality_level == "poor":
        processing_time = 55.0
        peak_memory = 9500.0
    else:  # medium
        processing_time = 35.0
        peak_memory = 7000.0
    
    performance_metrics = ProcessingPerformanceMetrics(
        processing_start_time=datetime.now() - timedelta(minutes=processing_time),
        processing_end_time=datetime.now(),
        total_processing_time_minutes=processing_time,
        stage_times={'download': 300, 'processing': processing_time * 60 - 600, 'upload': 300},
        slowest_stage='processing',
        slowest_stage_time=processing_time * 60 - 600,
        rows_per_minute=int(100000 / processing_time),
        mb_per_minute=50.0,
        processing_efficiency_score=0.8 if quality_level == "good" else 0.4,
        peak_memory_mb=peak_memory,
        final_memory_mb=peak_memory * 0.7,
        memory_reduction_mb=peak_memory * 0.3,
        memory_per_row_kb=(peak_memory * 1024) / 100000,
        memory_at_stages={'download': peak_memory * 0.3, 'processing': peak_memory, 'upload': peak_memory * 0.5},
        component_versions={'weighted_labeling': '3.0', 'feature_engineering': '2.1'},
        processing_methods={'labeling': 'vectorized', 'features': 'optimized'}
    )
    
    # Create sample rollover events
    rollover_events = []
    if quality_level == "poor":
        # Add many rollover events for poor quality
        for i in range(8):
            rollover_events.append(RolloverEvent(
                timestamp=datetime.now() - timedelta(days=i*3),
                price_gap=25.0 + i*5,
                bars_affected=6,
                gap_direction='up' if i % 2 == 0 else 'down'
            ))
    elif quality_level == "medium":
        # Add moderate rollover events
        for i in range(3):
            rollover_events.append(RolloverEvent(
                timestamp=datetime.now() - timedelta(days=i*7),
                price_gap=22.0,
                bars_affected=6,
                gap_direction='up' if i % 2 == 0 else 'down'
            ))
    else:  # good
        # Add minimal rollover events
        rollover_events.append(RolloverEvent(
            timestamp=datetime.now() - timedelta(days=15),
            price_gap=21.0,
            bars_affected=6,
            gap_direction='up'
        ))
    
    total_rollover_events = len(rollover_events)
    bars_excluded_rollover = sum(event.bars_affected for event in rollover_events)
    rollover_affected_percentage = (bars_excluded_rollover / int(100000 * retention_rate)) * 100
    
    # Create the statistics object
    stats = MonthlyProcessingStatistics(
        month_str=month_str,
        processing_date=datetime.now(),
        data_quality=data_quality,
        rollover_events=rollover_events,
        total_rollover_events=total_rollover_events,
        bars_excluded_rollover=bars_excluded_rollover,
        rollover_affected_percentage=rollover_affected_percentage,
        mode_statistics=mode_statistics,
        feature_statistics=feature_statistics,
        performance_metrics=performance_metrics,
        overall_quality_score=0.0,  # Will be calculated
        requires_reprocessing=False,  # Will be determined
        reprocessing_reasons=[],  # Will be populated
        processing_successful=quality_level != "poor",
        processing_errors=['critical_error'] if quality_level == "poor" else [],
        processing_warnings=warnings
    )
    
    # Calculate quality score using the quality scorer
    quality_scorer = QualityScorer()
    stats.overall_quality_score = quality_scorer.calculate_quality_score(stats)
    stats.requires_reprocessing, stats.reprocessing_reasons = quality_scorer.detect_reprocessing_requirements(stats)
    
    return stats


def test_quality_scorer():
    """Test the quality scoring algorithm"""
    print("üß™ Testing Quality Scorer...")
    
    quality_scorer = QualityScorer()
    
    # Test with good quality data
    good_stats = create_sample_monthly_statistics("2024-01", "good")
    good_score = quality_scorer.calculate_quality_score(good_stats)
    print(f"   Good quality score: {good_score:.3f}")
    assert good_score >= 0.7, f"Good quality score should be >= 0.7, got {good_score}"
    
    # Test with poor quality data
    poor_stats = create_sample_monthly_statistics("2024-02", "poor")
    poor_score = quality_scorer.calculate_quality_score(poor_stats)
    print(f"   Poor quality score: {poor_score:.3f}")
    assert poor_score <= 0.5, f"Poor quality score should be <= 0.5, got {poor_score}"
    
    # Test with medium quality data
    medium_stats = create_sample_monthly_statistics("2024-03", "medium")
    medium_score = quality_scorer.calculate_quality_score(medium_stats)
    print(f"   Medium quality score: {medium_score:.3f}")
    assert 0.5 < medium_score < 0.9, f"Medium quality score should be 0.5-0.9, got {medium_score}"
    
    print("   ‚úÖ Quality scorer tests passed")


def test_reprocessing_detection():
    """Test automated reprocessing detection"""
    print("üß™ Testing Reprocessing Detection...")
    
    quality_scorer = QualityScorer()
    
    # Test good quality - should not require reprocessing
    good_stats = create_sample_monthly_statistics("2024-01", "good")
    requires_reprocessing, reasons = quality_scorer.detect_reprocessing_requirements(good_stats)
    print(f"   Good quality reprocessing: {requires_reprocessing} (reasons: {len(reasons)})")
    assert not requires_reprocessing, "Good quality should not require reprocessing"
    
    # Test poor quality - should require reprocessing
    poor_stats = create_sample_monthly_statistics("2024-02", "poor")
    requires_reprocessing, reasons = quality_scorer.detect_reprocessing_requirements(poor_stats)
    print(f"   Poor quality reprocessing: {requires_reprocessing} (reasons: {len(reasons)})")
    assert requires_reprocessing, "Poor quality should require reprocessing"
    assert len(reasons) > 0, "Should have reprocessing reasons"
    
    print("   ‚úÖ Reprocessing detection tests passed")


def test_monthly_quality_validation():
    """Test monthly quality validation"""
    print("üß™ Testing Monthly Quality Validation...")
    
    # Test with good quality data
    good_stats = create_sample_monthly_statistics("2024-01", "good")
    validation_results = validate_monthly_quality(good_stats)
    print(f"   Good quality validation: {validation_results['overall_valid']}")
    assert validation_results['overall_valid'], "Good quality should pass validation"
    assert len(validation_results['failed_checks']) == 0, "Should have no failed checks"
    
    # Test with poor quality data
    poor_stats = create_sample_monthly_statistics("2024-02", "poor")
    validation_results = validate_monthly_quality(poor_stats)
    print(f"   Poor quality validation: {validation_results['overall_valid']}")
    assert not validation_results['overall_valid'], "Poor quality should fail validation"
    assert len(validation_results['failed_checks']) > 0, "Should have failed checks"
    assert len(validation_results['recommendations']) > 0, "Should have recommendations"
    
    print("   ‚úÖ Monthly quality validation tests passed")


def test_quality_report_generation():
    """Test quality report generation"""
    print("üß™ Testing Quality Report Generation...")
    
    # Test monthly report generation
    stats = create_sample_monthly_statistics("2024-01", "good")
    report = create_monthly_quality_report(stats)
    
    assert "Monthly Processing Quality Report" in report, "Report should have title"
    assert "2024-01" in report, "Report should include month"
    assert "Overall Quality Score" in report, "Report should include quality score"
    assert "Mode Statistics" in report, "Report should include mode statistics"
    assert "Feature Quality" in report, "Report should include feature quality"
    
    print(f"   Monthly report generated: {len(report)} characters")
    
    # Test saving report to file
    with tempfile.TemporaryDirectory() as temp_dir:
        report_file = save_monthly_report(stats, temp_dir)
        assert Path(report_file).exists(), "Report file should be created"
        
        with open(report_file, 'r') as f:
            saved_content = f.read()
        
        assert len(saved_content) > 0, "Saved report should have content"
        print(f"   Report saved to: {report_file}")
    
    print("   ‚úÖ Quality report generation tests passed")


def test_multi_month_reporting():
    """Test multi-month summary reporting"""
    print("üß™ Testing Multi-Month Reporting...")
    
    # Create sample data for multiple months
    monthly_stats = [
        create_sample_monthly_statistics("2024-01", "good"),
        create_sample_monthly_statistics("2024-02", "medium"),
        create_sample_monthly_statistics("2024-03", "poor"),
        create_sample_monthly_statistics("2024-04", "good"),
        create_sample_monthly_statistics("2024-05", "medium")
    ]
    
    # Test multi-month report generator
    report_generator = MultiMonthReportGenerator()
    for stats in monthly_stats:
        report_generator.add_monthly_stats(stats)
    
    summary_report = report_generator.generate_summary_report()
    
    assert "Multi-Month Processing Summary Report" in summary_report, "Should have summary title"
    assert "5" in summary_report, "Should mention 5 months"
    assert "Quality Score Distribution" in summary_report, "Should have quality distribution"
    assert "Performance Trends" in summary_report, "Should have performance trends"
    assert "Reprocessing Recommendations" in summary_report, "Should have reprocessing recommendations"
    
    print(f"   Summary report generated: {len(summary_report)} characters")
    
    # Test trend analysis
    trend_analysis = report_generator.generate_trend_analysis()
    assert 'trends' in trend_analysis, "Should have trends data"
    assert 'trend_analysis' in trend_analysis, "Should have trend analysis"
    assert len(trend_analysis['trends']['months']) == 5, "Should have 5 months of data"
    
    print(f"   Trend analysis generated with {len(trend_analysis['trends'])} metrics")
    
    # Test saving summary report
    with tempfile.TemporaryDirectory() as temp_dir:
        summary_file = save_summary_report(monthly_stats, temp_dir)
        assert Path(summary_file).exists(), "Summary file should be created"
        print(f"   Summary report saved to: {summary_file}")
    
    print("   ‚úÖ Multi-month reporting tests passed")


def test_statistics_serialization():
    """Test statistics serialization to JSON"""
    print("üß™ Testing Statistics Serialization...")
    
    stats = create_sample_monthly_statistics("2024-01", "good")
    
    # Test JSON serialization
    json_str = stats.to_json()
    assert len(json_str) > 0, "JSON string should not be empty"
    
    # Test that JSON is valid
    parsed_json = json.loads(json_str)
    assert parsed_json['month_str'] == "2024-01", "JSON should contain month"
    assert 'overall_quality_score' in parsed_json, "JSON should contain quality score"
    assert 'mode_statistics' in parsed_json, "JSON should contain mode statistics"
    
    print(f"   JSON serialization: {len(json_str)} characters")
    
    # Test saving to file
    with tempfile.TemporaryDirectory() as temp_dir:
        json_file = Path(temp_dir) / "test_stats.json"
        success = stats.save_to_file(json_file)
        assert success, "Should successfully save to file"
        assert json_file.exists(), "JSON file should exist"
        
        with open(json_file, 'r') as f:
            saved_json = json.load(f)
        
        assert saved_json['month_str'] == "2024-01", "Saved JSON should match original"
        print(f"   Statistics saved to: {json_file}")
    
    print("   ‚úÖ Statistics serialization tests passed")


def test_statistics_collector_integration():
    """Test integration with MonthlyStatisticsCollector"""
    print("üß™ Testing Statistics Collector Integration...")
    
    collector = MonthlyStatisticsCollector("2024-01")
    
    # Test data flow recording
    collector.record_data_flow('raw', 100000)
    collector.record_data_flow('cleaned', 95000)
    collector.record_data_flow('rth', 38000)
    collector.record_data_flow('final', 35000)
    
    assert collector.raw_bars == 100000, "Should record raw bars"
    assert collector.final_bars == 35000, "Should record final bars"
    
    # Test rollover event recording
    collector.record_rollover_event(
        datetime.now(),
        price_gap=22.5,
        bars_affected=6,
        gap_direction='up'
    )
    
    assert len(collector.rollover_events) == 1, "Should record rollover event"
    
    # Test stage timing
    collector.start_stage('processing')
    import time
    time.sleep(0.1)  # Small delay for testing
    collector.end_stage('processing', memory_mb=5000.0)
    
    assert 'processing' in collector.stage_times, "Should record stage time"
    assert collector.peak_memory_mb == 5000.0, "Should record peak memory"
    
    # Test component version recording
    collector.record_component_version('weighted_labeling', '3.0')
    collector.record_processing_method('labeling', 'vectorized')
    
    assert collector.component_versions['weighted_labeling'] == '3.0', "Should record component version"
    assert collector.processing_methods['labeling'] == 'vectorized', "Should record processing method"
    
    print("   ‚úÖ Statistics collector integration tests passed")


def run_comprehensive_test():
    """Run comprehensive test of enhanced statistics logging system"""
    print("üöÄ Testing Enhanced Statistics Logging and S3 Metadata System")
    print("=" * 70)
    
    try:
        # Run all tests
        test_quality_scorer()
        test_reprocessing_detection()
        test_monthly_quality_validation()
        test_quality_report_generation()
        test_multi_month_reporting()
        test_statistics_serialization()
        test_statistics_collector_integration()
        
        print("=" * 70)
        print("‚úÖ ALL TESTS PASSED - Enhanced Statistics Logging System Working Correctly")
        print()
        print("üìä System Features Validated:")
        print("   ‚Ä¢ Comprehensive statistics data model")
        print("   ‚Ä¢ Quality scoring algorithm with multiple factors")
        print("   ‚Ä¢ Automated reprocessing detection")
        print("   ‚Ä¢ Monthly quality validation")
        print("   ‚Ä¢ Quality report generation")
        print("   ‚Ä¢ Multi-month summary reporting")
        print("   ‚Ä¢ Trend analysis across months")
        print("   ‚Ä¢ JSON serialization for S3 storage")
        print("   ‚Ä¢ Statistics collector integration")
        print()
        print("üéØ Task 4 Implementation Complete:")
        print("   ‚úÖ 4.1 Comprehensive statistics data model")
        print("   ‚úÖ 4.2 Enhanced S3 metadata and storage")
        print("   ‚úÖ 4.3 Quality scoring and validation")
        print("   ‚úÖ 4.4 Monthly quality reporting system")
        
        return True
        
    except Exception as e:
        print(f"‚ùå TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_comprehensive_test()
    sys.exit(0 if success else 1)