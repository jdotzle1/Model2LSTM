#!/bin/bash
# EC2 Deployment Script for Weighted Labeling Pipeline
# Generated by final integration test

set -e  # Exit on any error

echo "=== ES WEIGHTED LABELING PIPELINE DEPLOYMENT ==="
echo "Instance: $(curl -s http://169.254.169.254/latest/meta-data/instance-type)"
echo "Date: $(date)"

# 1. Update system and install dependencies
echo "Step 1: Installing system dependencies..."
sudo yum update -y
sudo yum install -y python3 python3-pip git htop

# 2. Install Python packages
echo "Step 2: Installing Python packages..."
pip3 install --user pandas numpy xgboost scikit-learn pyarrow boto3 pytz databento psutil

# 3. Set up environment variables
echo "Step 3: Setting up environment..."
export S3_BUCKET="${S3_BUCKET:-your-es-data-bucket}"
export S3_DBN_PREFIX="${S3_DBN_PREFIX:-raw/dbn/}"
export S3_OUTPUT_PREFIX="${S3_OUTPUT_PREFIX:-processed/weighted_labeling/}"

# 4. Create working directory
echo "Step 4: Creating working directory..."
mkdir -p /tmp/es_weighted_pipeline
cd /tmp/es_weighted_pipeline

# 5. Download pipeline code (assuming it's in S3 or Git)
echo "Step 5: Downloading pipeline code..."
# aws s3 cp s3://your-code-bucket/pipeline.tar.gz .
# tar -xzf pipeline.tar.gz

# 6. Validate integration
echo "Step 6: Running integration validation..."
python3 aws_setup/validate_ec2_integration.py --test-s3

# 7. Run complete pipeline
echo "Step 7: Starting weighted labeling pipeline..."
python3 aws_setup/ec2_weighted_labeling_pipeline.py \
    --bucket "$S3_BUCKET" \
    2>&1 | tee pipeline.log

echo "=== PIPELINE DEPLOYMENT COMPLETE ==="
echo "Check pipeline.log for detailed results"
echo "Results uploaded to s3://$S3_BUCKET/$S3_OUTPUT_PREFIX"
